
\documentclass{article}
\usepackage{amsmath}

\begin{document}

\begin{titlepage}
    \centering
    \vspace*{2cm}
    {\Huge\bfseries Report\par}
    \vspace{2cm}
    {\Large\itshape Homework 1\par}
    \vspace{0.5cm}
    {\large\itshape Goal: Develop models for 10-class classification problems with medium and large input space\par}
    \vfill
    {\Large Andrea Massignan\par}
    \vfill
    {\large\today\par}
\end{titlepage}

\section*{Project Overview}

The goal is to provide two different solution for a 10-class classification problem. 

\section{Dataset}

The datasets used in this project consists in training sets and blind test sets. 
The training sets are composed by 50000 samples, each one with 100 features for Dataset 1 and 1000 features for Dataset 2. 
The blind test sets are composed by 10000 samples, each one with 100 features for Dataset 1 and 1000 features for Dataset 2. 
The labels are 10, one for each class.

\begin{itemize}
  \item \texttt{X\_train}: 50000 samples for each dataset.
  \item \texttt{n\_features}: 100 for Dataset 1 and 1000 for Dataset 2.

\end{itemize}

\section{Data Exploration}
The datasets, that are in a csv file, the elements $V^{(i)}$ belonging 
to the $X$ column are feature vectors that represent the input data.
\newline
\newline
The Y column contains the associated labels $C_k$(k=0,...9).
\newline
\newline
Thus, each line i in column X is a feature vector structured as:
\newline
\newline
[$V^{(i)}_1$, $V^{(i)}_2$, ..., $V^{(i)}_j$, ... , $V^{(i)}_d$]
\newline
\newline
Where d is the size of the feature vector with sizes described before.
\newline
\newline
And $i=1,...,n$, where n is the number of samples in the dataset as specified before.
\newline
\newline
These are loaded with an helper function called \texttt{load\_data} that returns the training set in the X matrix and the labels in the Y vector.
\newline
\newline
Then the training set is split into \texttt{X\_train} and \texttt{Y\_train} with a test size of $0.2$ and different random states.
\newline
\newline
Since the dataset is already vectorized, no further preprocessing is needed at the moment.

\section{Metrics}
The metrics used to evaluate the models are the accuracy and the confusion matrix.
\newline

\subsection{Precision}
Precision is a measure of the accuracy of the positive predictions made by a model. It is the ratio of true positive predictions to the total number of positive predictions made by the model (both true positives and false positives). Precision is calculated using the formula:
\[
\text{Precision} = \frac{\text{True Positives}}{\text{True Positives + False Positives}}
\]
Precision provides insight into the model's ability to avoid making false positive predictions. A high precision indicates that when the model predicts a positive class, it is likely to be correct.

\subsection{Recall (Sensitivity or True Positive Rate)}
Recall measures the ability of a model to correctly identify all relevant instances of the positive class. It is the ratio of true positive predictions to the total number of actual positive instances in the dataset (including both true positives and false negatives). Recall is calculated using the formula:
\[
\text{Recall} = \frac{\text{True Positives}}{\text{True Positives + False Negatives}}
\]
Recall is especially important when the cost of false negatives (missing a positive instance) is high, as it focuses on minimizing false negatives.

\subsection{F1-Score}
The F1-score is the harmonic mean of precision and recall. It provides a balanced measure that considers both false positives and false negatives. The F1-score is calculated using the formula:
\[
\text{F1-Score} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision + Recall}}
\]
F1-score is useful when there is an uneven class distribution or when false positives and false negatives have different consequences.

\subsection{Support}
Support refers to the number of actual instances of each class in the dataset. It is the count of true positive and true negative instances for each class. Support is not directly involved in the calculation of precision, recall, or F1-score, but it provides context by showing how many instances belong to each class.


\end{document}
